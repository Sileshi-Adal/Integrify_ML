{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.08 Assignment on -  Regression, Maximum Likelihood Estimation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What is a likelihood function? Also add a formula and explain what it means.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T06:17:47.729357Z",
     "start_time": "2019-08-06T06:17:47.712005Z"
    }
   },
   "source": [
    "Likelihood function is the probability of the observed data as a function of unknown parameter.\n",
    "Mathematically, \n",
    "\\begin{equation}\n",
    "L(\\theta, \\boldsymbol{x})=\\prod_{i=1}^{n} f_{i}\\left(x_{i}\\right)=\\prod_{i=1}^{n} \\theta_{i}^{x_{i}}\\left(1-\\theta_{i}\\right)^{1-x_{i}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where ,\n",
    "\\begin{equation}\n",
    "L(\\theta) = likelihood-function\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "x = observed-data \n",
    "\\end{equation} \n",
    "\n",
    "\\begin{equation}\n",
    "\\theta= unknown-parameter\n",
    "\\end{equation}\n",
    "\n",
    "    1.Unlike pdf, likelihoods aren’t normalized. The area under curve does not have to be 1.\n",
    "    2.We also use likelihoods to generate estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What is Maximum Likelihood estimation (MLE) ? Can you give an example?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T06:27:16.321184Z",
     "start_time": "2019-08-06T06:27:16.296715Z"
    }
   },
   "source": [
    "The value of parameter (\\theta), which maximizes the likelihood function (L($\\theta$))\n",
    "is known as maximum likelihood criterion. One example of this is by finding the critical points, as maximum lies at that point, by taking derivative of likelihood function and setting it to zero.  However, if it cannot be solved analytically, we should use optimization algorithm to find the maximum likelihood.\n",
    "\n",
    "One example of maximum likelihood is flipping a coin certain(let say 100) times  and finding the maximum likelihood of having the head or tail on the next flip given the data of prior flip.\n",
    "Mathematically maximum likelihood estimate for Head is, \n",
    "\\begin{equation}\n",
    "\\hat{\\theta}_{\\mathrm{ML}}=\\frac{N_{H}}{N_{H}+N_{T}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Write out MSE loss for linear regression. Could we also use this loss for classification?** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE measures the average squared difference between an observation’s actual and predicted values. The output is a single number representing the cost, or score, associated with our current set of weights.\n",
    "Given our simple linear equation $y=mx+b$, we can calculate MSE as:\n",
    "$$M S E=\\frac{1}{N} \\sum_{i=1}^{n}\\left(y_{i}-\\left(m x_{i}+b\\right)\\right)^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Write out the Maximum likelihood Estimation for linear regression. How is this related to the MSE loss for linear regression derived in the last point? Derive the relation between them.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume the data is described by the linear model $\\mathbf{y} = \\mathbf{wX} + \\epsilon$, where $\\epsilon_i \\sim N(\\epsilon_i; 0,\\sigma^2_e)$. Assume $\\sigma^2_e$ is known and the datapoints are i.i.d.<b>\n",
    "The log likelihood of our model is\n",
    "\n",
    "$\\log p(\\mathbf{y}|\\mathbf{X, w}) = \\sum_{i=1}^N \\log p(y_i | \\mathbf{x_i, \\theta})$<b>\n",
    "\n",
    "But since the noise $\\epsilon$ is normally distributed, the likelihood is just<b>\n",
    "\n",
    "$\\begin{aligned} \\log p(\\mathbf{y} | \\mathbf{X}, \\mathbf{w}) &=\\sum_{i=1}^{N} \\log N\\left(y_{i} ; \\mathbf{x}_{\\mathbf{i}} \\mathbf{w}, \\sigma^{2}\\right) \\\\ &=\\sum_{i=1}^{N} \\log \\frac{1}{\\sqrt{2 \\pi \\sigma_{e}^{2}}} \\exp \\left(-\\frac{\\left(y_{i}-\\mathbf{x}_{\\mathbf{i}} \\mathbf{w}\\right)^{2}}{2 \\sigma_{c}^{2}}\\right) \\\\ &=-\\frac{N}{2} \\log 2 \\pi \\sigma_{e}^{2}-\\sum_{i=1}^{N} \\frac{\\left(y_{i}-\\mathbf{x}_{\\mathbf{i}} \\mathbf{w}\\right)^{2}}{2 \\sigma_{e}^{2}} \\end{aligned}$<b>\n",
    "\n",
    "Where N is the number of datapoints. The value of w that maximises equation is the maximum likelihood estimate w^MSE. That is,<b>\n",
    " \n",
    "$\\mathbf{w}_{M L E}=\\arg \\max _{\\mathbf{w}}-\\sum_{i=1}^{N}\\left(y_{i}-\\mathbf{x}_{\\mathbf{i}} \\mathbf{w}\\right)^{2}$<b>\n",
    "\n",
    "Recall that maximising a function is the same as minimising its negative, so we can rewrite equation as:<b>\n",
    "\n",
    "$\\mathbf{w}_{M L E}=\\arg \\min _{\\mathbf{w}} \\frac{1}{N} \\sum_{i=1}^{N}\\left(y_{i}-\\mathbf{x}_{\\mathbf{i}} \\mathbf{w}\\right)^{2}$\n",
    "$=\\arg \\min _{\\mathbf{w}} \\operatorname{MSE}$<b>\n",
    "    \n",
    "which is exactly the mean squared error (MSE) cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Write out the likelihood function for linear classification. What is the drawback of using MSE loss here?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is coverd on the last quetion 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Can gradient descent be used to find the parameters for linear regression? What about linear classification? Why?**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T07:06:01.874624Z",
     "start_time": "2019-08-06T07:06:01.839340Z"
    }
   },
   "source": [
    "Yes, gradient descent be used to find the parameters for linear regression. Since gradient descent optimization algorithm is great in finding the parameter (coefficient) if it cannot be solved using linear algebra, We can definitely use gradient descent and find the optimal parametric value of the regression line by finding the optimal value that minimizes the error function.\n",
    "It can also be used in linear classification as the cost function for linear classification is similar to linear regression and both are derived using the principle of maximum likelihood estimation.\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\theta)=\\frac{1}{m} \\sum_{i=1}^{m} \\frac{1}{2}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "we use the gradient descent algorithm and update the parameter taking the specific learning  rate and  the repititive update yields the minimal cost function and thus optimal parameter for it to occur. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. What are normal equations? Is it the same as least squares? Explain. You may refer to this** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal equation is a technique for computing coefficients for Multivariate Linear Regression. Normal equation is used to find the best fit solution to the regression problem using least square method i.e. used to find the optimal value of unknown paramters.\n",
    "\n",
    "Mathematically,\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\mathbf{w})=\\|\\mathbf{e}\\|^{2}=\\|\\mathbf{y}-X \\mathbf{w}\\|^{2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "w=\\left(X^{T} X\\right)^{-1} X^{T} y\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus(e). What is regularization in regression? What is the mathematical relation between regularization and normal equations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Is feature scaling needed for linear regression when using gradient descent?  Why or why not?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, feature scaling is needed for linear regression beecause it speeds up gradient descent by scaling. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven. It speeds up gradient descent by making it require fewer iterations to get to a good solution. It is necessary to prevent gradient descent from getting stuck in local optima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Write out the MLE approach for logistic regression. How is this related to the binary cross-entropy? You can use this reference to answer this question. Video Reference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "With 0 and 1 being the only possible outcomes, the probability of observing outcome y=0 is simply (1-p):\n",
    "\n",
    "$$\n",
    "p(y=o | \\mathbf{B}, \\mathbf{X})=p^{o}(1-p)^{1-o}\n",
    "$$\n",
    "\n",
    "The likelihood function is given by the product of all individual probabilities:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}=\\prod_{i=1}^{n} p\\left(y=y_{i} | \\mathbf{B}_{i}, \\mathbf{X}_{i}\\right)=\\prod_{i=1}^{n} p_{i}^{y_{i}}\\left(1-p_{i}\\right)^{1-y_{i}}\n",
    "$$\n",
    "\n",
    "It’s easier to maximize the log-likelihood:\n",
    "\n",
    "$$\n",
    "\\ln \\mathcal{L}=\\sum_{i=1}^{n}\\left(y_{i} \\ln p_{i}+\\left(1-y_{i}\\right) \\ln \\left(1-p_{i}\\right)\\right)\n",
    "$$\n",
    "Thus  maximum liklihood estimation yields a familiar loss function (cross-entropy in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the basics of statistical machine learning, the loss function is the negative-log-likelihood (NLL) of the model. To find the best parameters, we need to minimize this NLL loss. The MSE is the NLL for the linear regression model (with Gaussian likelihood). But for the logistic regression model, the NLL is the cross-entropy (with Bernoulli likelihood). So minimizing the MSE loss for a logistic regression model is minimizing something else other than the NLL of that model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
