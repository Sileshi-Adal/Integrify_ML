{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\t1. Explain in your own words what effect does the choice of Covariance matrix have on the Bivariate Gaussian (compare spherical, elliptical). What does it mean when the covariance matrix is not diagonal? [PYTHON + DEMO] Normal Distribution (1-D and 2-D)\n",
    "```\n",
    "1.) To understand how to prove that the covariance of two features converge to the same proof of there being one feature, we first have to understand a few key pieces of information.\n",
    "\n",
    "\n",
    "$1.) 1D : f(x)=1 / \\sqrt((2 \\pi) \\wedge k *| | \\Sigma |) \\exp \\left[-1 / 2(x-\\mu) \\wedge T \\Sigma^{\\wedge}-1(x-\\mu)\\right]\n",
    "$\n",
    "\n",
    "$\\hspace{1cm}f(x, y)=\\frac{1}{(2 \\pi)^{k}|\\Sigma|} \\exp \\left[\\frac{1}{2\\left(1-\\rho^{2}\\right)}\\left(\\frac{x-\\mu_{x}}{\\sigma_{x}^{2}}+\\frac{\\left(y-\\mu_{y}\\right)}{\\sigma_{y}^{2}}+\\frac{\\rho\\left(x-\\mu_{x}\\right)\\left(y-\\mu_{y}\\right)}{\\sigma_{x} \\sigma_{y}}\\right)\\right]\n",
    "$\n",
    "\n",
    "2.) $\\Sigma=\\left[\\begin{array}{cc}{\\sigma_{x}^{2}} & {-\\zeta \\sigma_{x} \\sigma_{y}} \\\\ {-\\zeta \\sigma_{x} \\sigma_{y}} & {_{\\sigma_{y}^{2}}}\\end{array}\\right] D(\\Sigma)=\\left(\\sigma_{x}^{2} \\sigma_{y}^{2}\\right)-\\left(\\zeta \\sigma_{x} \\sigma_{y}\\right)=\\left(\\sigma_{x}^{2} \\sigma_{y}^{2}\\right)\\left(1-\\zeta^{2}\\right) $\n",
    "\n",
    "3.) $\\Sigma^{-} 1=\\left[\\begin{array}{cc}{\\sigma_{y}^{2}} & {-\\zeta \\sigma_{x} \\sigma_{y}} \\\\ {-\\zeta \\sigma_{x} \\sigma_{y}} & {\\sigma_{x}^{2}}\\end{array}\\right] \\frac{1}{\\left(\\left(\\sigma_{x}^{2} \\sigma_{y}^{2}\\right)\\left(1-\\zeta^{2}\\right)\\right)}$\n",
    "\n",
    "4.) $(\\overline{x}-\\overline{\\mu})^{T}=\\left[x-\\mu_{x} \\quad y-\\mu_{y}\\right]$\n",
    "\n",
    "5.) $\\left(\\frac{-1}{2}\\right)\\left[x-\\mu_{x} \\quad y-\\mu_{y}\\right]\\left[\\begin{array}{cc}{\\sigma_{y}^{2}} & {-\\zeta \\sigma_{x} \\sigma_{y}} \\\\ {-\\zeta \\sigma_{x} \\sigma_{y}} & {\\sigma_{x}^{2}}\\end{array}\\right] \\frac{1}{\\left(\\sigma_{x}^{2} \\sigma_{y}^{2}\\right)\\left(1-\\zeta^{2}\\right)}\\left[\\begin{array}{l}{x-\\mu_{x}} \\\\ {y-\\mu_{y}}\\end{array}\\right]$\n",
    "\n",
    "$$\\left(\\frac{-1}{2}\\right)\\left[x-\\mu_{x} \\quad y-\\mu_{y}\\right]\\left[\\begin{array}{cc}{\\sigma_{y}^{2}\\left(x-\\mu_{x}\\right)} & {\\left(-\\zeta \\sigma_{x} \\sigma_{y}\\right)\\left(y-\\mu_{y}\\right)} \\\\ {\\left(-\\zeta \\sigma_{x} \\sigma_{y}\\right)\\left(\\left(y-\\mu_{y}\\right)\\right)} & {\\left(\\sigma_{x}^{2}\\right)\\left(x-\\mu_{x}\\right)}\\end{array}\\right] \\frac{1}{\\left(\\sigma_{x}^{2} \\sigma_{y}^{2}\\right)\\left(1-\\zeta^{2}\\right)}$\n",
    "$$\n",
    "$ $\n",
    "$\\hspace{1cm}\\frac{-1}{2\\left(1-\\zeta^{2}\\right)}\\left(\\frac{\\left(y-\\mu_{y}\\right)^{2}}{\\sigma_{y}^{2}}-\\frac{2 \\zeta\\left(y-\\mu_{y}\\right)\\left(x-\\mu_{x}\\right)}{\\sigma_{x} \\sigma_{y}}+\\frac{\\left(x-\\mu_{x}\\right)^{2}}{\\sigma_{x}^{2}}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```2. Do the linear transformation assignment here```\n",
    "\n",
    "$2.\\hspace{1mm} X \\hspace{1mm} \\sim \\hspace{1mm} N(\\mu_n, \\Sigma_n) \\hspace{1mm}$ and Y = Ax + b where $b\\sim{N}(\\mu = 0, \\Sigma_b)$. Since x and b are from normal distribution, y and $(x, y)^T$ are also from normal distribution. To find the parameters, we only need to know its mean and variance.\n",
    "$ \\\\ Then, \\hspace{1mm}y = Ax + b \\\\ \\hspace{1cm} Mean: E(y) \\ = AE(x) + E(b) = AE(x) = A\\mu_x \\\\ \\hspace{1cm} Variance: \\Sigma_y = E[y-E(y)]^T[(y-E(y) + var(b))] \\\\ \\hspace{3.45cm} = E[(Ax + b - AE(x) - b)^T(Ax + b - AE(x) - b)] + \\Sigma_b \\\\ \\hspace{3.45cm} = E[(Ax - AE(x)^T(Ax - AE(x)] + \\Sigma_b \\\\ \\hspace{3.45cm} = A^TE[(x-E(x)^T)(x-E(x))]A + \\Sigma_b \\\\ \\hspace{3.45cm} = A^T\\Sigma_xA + \\Sigma_b $\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say that $y = Ax + b$ is a normal random vector with mean: \\$$A\\mu_{n}\\\\$$\n",
    "\n",
    "\n",
    "Variance:\n",
    "\n",
    "\\$$A^T\\Sigma_{x}A + \\Sigma_{b}\\\\$$\n",
    "\n",
    "The joint distribution of $(x, y)^T$ is also a normal distribution. $its \\hspace{1mm} mean, \\mu_{ny} = \\begin{pmatrix} \\mu_{n} \\\\ A\\mu_{n} \\end{pmatrix}$\n",
    "\\$$cov(x, y) = E[xy]^T - E[x]E[y]^T\\\\$$\n",
    "\\$$= E[x(Ax + b)] -\\mu_{n}A^T\\mu_{n}^T\\\\$$\n",
    "\\$$=E[xA_{x} + b_{x}]^T - \\mu_{n}A^T\\mu_{n}^T = \\Sigma_{n}A^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\t3. What is the meaning of Mahalanobis distance? What is the relation of this to the eigenvalues of the Covariance matrix? Draw a sketch either in Python or by hand for the Bivariate case (K=2)\n",
    "```\n",
    "\n",
    "Q3. Mahalanobis distance is the measure of the distance between a point Z and a distribution D.\n",
    "The relation of the Mahalanobis to the eigenvalues (Eigen vector) of the Covariance matrix is used to re-scaling to remove the Covariance in order to get the Euclidian distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image_1](20190719_145030.jpg)\n",
    "![image_2](20190719_145039.jpg)\n",
    "![image_3](20190719_145022.jpg)\n",
    "![image_4](20190719_145055.jpg)\n",
    "![image_5](20190719_145100.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
